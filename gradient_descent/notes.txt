Gradient Descent
    * simultaneous update theta0, theta1, etc. based upon the partial derivative
    * alpha (learning rate) tradeoff:
        * too small -> takes a long time
        * too big -> can fail to converg & diverge 
    * as we approach local minimum algorithim will automatically take smaller steps. At minimum theta will remain unchanged    
        

    Gradient descent for linear regression

    partial_deriv: 1/2m * summation(H0(xi) - yi) ^ 2
    partial_deriv: 1/2m * summation(theta0 + theta1xi - yi) ^ 2

    theta0 result: 1/m sum(h0xi - yi)
    theta1 result: 1/m sum(h0xi - yi)*xi

    linear regression will always have a 'bowl-shaped' convex function: 
        * will always converge to global optima

    otherwise known as batch because were computing over all training examples. The entire 'batch'

    can use normal equations method from linear algebra, but GD scales better to larger data

    practical tips:
        * feature scaling (normalization) can help GD converge faster. (cantours become less skewed / eliptical)
        * plot J(theta) as a function of No. of iterations 
            * J(theta) should decrease after every iteration
            * ^ can use this to choose a learning rate as well
            * To those alphatry .001, .003, .01, .03, .1, 1. Find one that is too larget and one that is too small
        

Stochastic Gradient Descent (fits 1 observation at a time)
    cost(theta, (xi,yi)) = 1/2 * (h0(xi) = yi)^2                 
    Jtrain(theta) = 1/m sum(cost(theta,(xi,yi))

    1. Randomly shuffle dataset
    2. Repeat {
        for i =1,...,,m {
            thetj = thetaj -alphta (h(xi) = yi)*xij
                for j=0,....,n) 
            }
        }

    convergence:
        compute hypothesis before updating theta
        every 1000 iterations (say) plot cost(theta, (xi, yi)) averaged over the last 1000 examples processed by algorithm
            need to choose how many examples to average over:

Mini Batch Gradient descent:
    b = mini-batch size; typically b = 2 to 10    
    * like gradient descent but with smaller 'batches'
        * b = 2 to 100

    * can partially parrellelize the summation with vectorization 


